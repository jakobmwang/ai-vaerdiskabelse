# LLM
En sprogmodel (LLM) alene er en matematisk funktion.

$y = f(x)$.

Når du skriver en besked til en chatbot, oversætter et program (en /tokenizer/) først dine ord til en serie af tal $x$ inden de sendes videre til sprogmodellen, den matematiske funktion $f(x)$.

Inde i $f(x)$ sker der enormt mange beregninger. Typisk tælles de i milliarderne afhængigt af sprogmodellens størrelse. På en GPU kan beregningerne køre sideløbende i stedet for at vente på hinanden, og derfor kan det gå virkelig hurtigt og skal måles i millisekunder.

Når alle beregningerne er kørt, kommer der i sidste ende ét tal $y$ ud af det. Dette tal oversættes direkte tilbage til et ord med samme ordbog, som blev brugt til at oversætte din besked til tal.

Det er derfor en sprogmodel svarer med ét ord ad gangen. Hvert af disse ord bliver løbende føjet til den oprindelige besked og kørt igennem $f(x)$ igen. Det giver endnu et nyt ord, og sådan kører det i ring, indtil sprogmodellen skriver et stopord, der betyder, at svaret er færdigt, og at programmet ikke skal kalde funktionen $f(x)$ yderligere.

Når sprogmodellen kan generere serier af ord, der fungerer som svar i en samtale, er det fordi, at den modtager hele samtalen i form af et manuskript, som den fortsætter. Det er på den måde, rammen dannes for en simpel chatbot.

I praksis virker det ved, at værdien for hvert enkelt ord beregnes, når sprogmodellens milliarder af interne værdier (ofte kaldet parametre eller vægte) på kryds og tværs ganges eller lægges til de værdier, som dine ord er oversat til.

Sprogmodellens interne værdier, som i øvrigt er helt almindelige decimaltal, er resultatet af dens træning, hvor de er blevet matematisk mikrojusteret trilliarder af gange, når $f(x)$ igen og igen har regnet på bidder af sin meget store mængde træningsdata. Efter træningen (også kaldet fitting; en model fittes eller tilpasses dataen) som kræver enormt store mængder regnekraft, er værdierne dog helt statiske, og der sker ingen justeringer, når den anvendes.

På den måde vil sprogmodellen altså generere et ord $y$, som i dens træningsdata med tilstrækkelig sandsynlighed følger den forudgående del af samtalen $x$. Så spørgsmålet er, hvilket ord der i modellens træningsdata sandsynligvis følger, og hvad der i træningsprocessen er brugt som definition på sandsynlighed?

Medmindre du får værdi af generisk tekst, har sprogmodellen altså brug for et lag mere. Den skal pakkes ind i en applikation, som kan styre den og stille værktøjer til rådighed for den.
